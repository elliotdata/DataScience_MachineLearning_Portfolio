{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection"
      ],
      "metadata": {
        "id": "bqGfQiva_jmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies and import\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zMEfZU95Bx3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytube google-api-python-client pandas slugify tqdm requests huggingface_hub sentence-transformers faiss-cpu unstructured -q"
      ],
      "metadata": {
        "id": "a4XDlG5k_oVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "import os\n",
        "import re\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "from pytube import YouTube"
      ],
      "metadata": {
        "id": "K0DkkHW8_owz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Directories"
      ],
      "metadata": {
        "id": "k-0RtcvpCXy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_dir = \"audio_files\"\n",
        "transcript_dir = \"transcripts\"\n",
        "\n",
        "os.makedirs(audio_dir, exist_ok=True)\n",
        "os.makedirs(transcript_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "bHCbSMC5_ozj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieving YT Channel videos"
      ],
      "metadata": {
        "id": "7alVmqnyCcxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YT API Client\n",
        "api_key = \"YT API\"\n",
        "youtube = build('youtube', 'v3', developerKey=api_key)"
      ],
      "metadata": {
        "id": "9h-7Hjg-_o2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace CHANNEL_ID with the actual ID of the YouTube channel\n",
        "channel_id = \"UCPjNBjflYl0-HQtUvOx0Ibw\" #Greg Isenberg YT Channel\n",
        "\n",
        "# Retrieve the channel's uploaded videos\n",
        "channel_videos = []\n",
        "next_page_token = None\n",
        "\n",
        "while True:\n",
        "    pl_request = youtube.search().list(\n",
        "        part=\"snippet\",\n",
        "        channelId=channel_id,\n",
        "        maxResults=50,\n",
        "        pageToken=next_page_token,\n",
        "        type=\"video\"\n",
        "    )\n",
        "    pl_response = pl_request.execute()\n",
        "\n",
        "    channel_videos.extend(pl_response[\"items\"])\n",
        "\n",
        "    next_page_token = pl_response.get(\"nextPageToken\")\n",
        "    if not next_page_token:\n",
        "        break"
      ],
      "metadata": {
        "id": "iS5YQPIf_o5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Video IDs and Titles"
      ],
      "metadata": {
        "id": "evkkZCVrCrUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists to store video IDs and titles\n",
        "video_ids = []\n",
        "video_titles = []\n",
        "\n",
        "# Iterate over each video in the channel\n",
        "for video in channel_videos:\n",
        "    # Extract video ID and append to the list\n",
        "    video_id = video['id']['videoId']\n",
        "    video_ids.append(video_id)\n",
        "\n",
        "    # Extract video title and append to the list\n",
        "    video_title = video['snippet']['title']\n",
        "    video_titles.append(video_title)\n",
        "\n",
        "# Create a DataFrame with video IDs and titles\n",
        "video_df = pd.DataFrame({'Video ID': video_ids, 'Video Title': video_titles})\n",
        "\n",
        "# Display the DataFrame\n",
        "print(video_df)"
      ],
      "metadata": {
        "id": "1_9QXz_LCq1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Audio Files"
      ],
      "metadata": {
        "id": "yO3HBbuTCw9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytube import YouTube\n",
        "import pandas as pd\n",
        "import slugify\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from pytube.exceptions import VideoUnavailable"
      ],
      "metadata": {
        "id": "RU69yGvICwJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to download high-quality WAV audio from YouTube video\n",
        "def download_high_quality_audio(video_id, video_title):\n",
        "    try:\n",
        "        yt = YouTube(f\"https://www.youtube.com/watch?v={video_id}\")\n",
        "        audio_stream = yt.streams.filter(only_audio=True, file_extension='mp4').order_by('abr').desc().first()\n",
        "        audio_stream.download(output_path=\"/content/audio_files\", filename=f\"{slugify.slugify(video_title)}.wav\")\n",
        "    except VideoUnavailable:\n",
        "        print(f\"The video with ID {video_id} is unavailable.\")"
      ],
      "metadata": {
        "id": "-ecqetUqC1vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(\"/content/audio_files\", exist_ok=True)\n",
        "\n",
        "# Assuming you have a dataframe video_df with video_ids and video_titles\n",
        "for index, row in tqdm(video_df.iterrows(), total=len(video_df), desc=\"Downloading Audio\"):\n",
        "    video_id = row['Video ID']\n",
        "    video_title = row['Video Title']\n",
        "    download_high_quality_audio(video_id, video_title)"
      ],
      "metadata": {
        "id": "EVDwXtErC3TY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#in-case you need to delete the directory and do it again. I was there. :)\n",
        "\n",
        "# Define the path to the folder\n",
        "folder_path = '/content/audio_files'\n",
        "\n",
        "# Check if the folder exists\n",
        "if os.path.exists(folder_path):\n",
        "    # Delete the folder and its contents\n",
        "    shutil.rmtree(folder_path)\n",
        "    print(\"Folder 'audio_files' deleted successfully.\")\n",
        "else:\n",
        "    print(\"Folder 'audio_files' does not exist.\")"
      ],
      "metadata": {
        "id": "Lr-GhYb7DDGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcribing Audio Files\n",
        "Get your free Assembly AI or paid."
      ],
      "metadata": {
        "id": "Z5bboXFyDVHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install assemblyai"
      ],
      "metadata": {
        "id": "e5dN0FN6DTwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transcription with confidence scores, speaker labels etc.\n",
        "import requests\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Your AssemblyAI API key\n",
        "api_key = 'ASSEMBLY API KEY'\n",
        "\n",
        "def transcribe_audio(file_path):\n",
        "    # Upload the audio file\n",
        "    with open(file_path, 'rb') as f:\n",
        "        upload_response = requests.post(\n",
        "            'https://api.assemblyai.com/v2/upload',\n",
        "            headers={'authorization': api_key},\n",
        "            data=f\n",
        "        ).json()\n",
        "\n",
        "    # Transcribe the audio file\n",
        "    transcript_response = requests.post(\n",
        "        'https://api.assemblyai.com/v2/transcript',\n",
        "        headers={'authorization': api_key},\n",
        "        json={\n",
        "            'audio_url': upload_response['upload_url'],\n",
        "            'iab_categories': True,\n",
        "            'speaker_labels': True\n",
        "        }\n",
        "    ).json()\n",
        "\n",
        "    # Poll for the transcript to be ready\n",
        "    while True:\n",
        "        result_response = requests.get(\n",
        "            f\"https://api.assemblyai.com/v2/transcript/{transcript_response['id']}\",\n",
        "            headers={'authorization': api_key}\n",
        "        ).json()\n",
        "        if result_response['status'] == 'completed':\n",
        "            return result_response\n",
        "        elif result_response['status'] == 'error':\n",
        "            raise Exception(f\"Transcription failed: {result_response}\")\n",
        "        time.sleep(10)\n",
        "\n",
        "# Use the function\n",
        "file_path = '/content/audio_files/elons-perfect-reaction-to-charlie-mungers-takedown.wav'\n",
        "result = transcribe_audio(file_path)\n",
        "\n",
        "# Save the result to a file\n",
        "os.makedirs(\"/content/transcripts\", exist_ok=True)\n",
        "with open(f\"/content/transcripts/{os.path.basename(file_path)}.json\", 'w') as f:\n",
        "    json.dump(result, f)\n",
        "\n",
        "print(\"Transcription saved successfully!\")"
      ],
      "metadata": {
        "id": "sSGRqkR7DLsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcribing a Single Audio File (Text Only)"
      ],
      "metadata": {
        "id": "vFhYip6_DlBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#transcription with texts only for individual file\n",
        "import requests\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Your AssemblyAI API key\n",
        "api_key = 'ASSEMBLY API KEY'\n",
        "\n",
        "def transcribe_audio(file_path):\n",
        "    # Upload the audio file\n",
        "    with open(file_path, 'rb') as f:\n",
        "        upload_response = requests.post(\n",
        "            'https://api.assemblyai.com/v2/upload',\n",
        "            headers={'authorization': api_key},\n",
        "            data=f\n",
        "        ).json()\n",
        "\n",
        "    # Transcribe the audio file\n",
        "    transcript_response = requests.post(\n",
        "        'https://api.assemblyai.com/v2/transcript',\n",
        "        headers={'authorization': api_key},\n",
        "        json={\n",
        "            'audio_url': upload_response['upload_url'],\n",
        "            'iab_categories': True,\n",
        "            'speaker_labels': True\n",
        "        }\n",
        "    ).json()\n",
        "\n",
        "    # Poll for the transcript to be ready\n",
        "    while True:\n",
        "        result_response = requests.get(\n",
        "            f\"https://api.assemblyai.com/v2/transcript/{transcript_response['id"
      ],
      "metadata": {
        "id": "w5y7P5EADjPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcription with Speaker Labels"
      ],
      "metadata": {
        "id": "834Hl-SOD7uC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "import concurrent.futures\n",
        "\n",
        "# Your AssemblyAI API key\n",
        "api_key = '60dceb143b3f4c898e1f2c70637f5d44'\n",
        "\n",
        "def transcribe_audio(file_path):\n",
        "    # Upload the audio file\n",
        "    with open(file_path, 'rb') as f:\n",
        "        upload_response = requests.post(\n",
        "            'https://api.assemblyai.com/v2/upload',\n",
        "            headers={'authorization': api_key},\n",
        "            data=f\n",
        "        ).json()\n",
        "\n",
        "    # Transcribe the audio file\n",
        "    transcript_response = requests.post(\n",
        "        'https://api.assemblyai.com/v2/transcript',\n",
        "        headers={'authorization': api_key},\n",
        "        json={\n",
        "            'audio_url': upload_response['upload_url'],\n",
        "            'iab_categories': True,\n",
        "            'speaker_labels': True\n",
        "        }\n",
        "    ).json()\n",
        "\n",
        "    # Poll for the transcript to be ready\n",
        "    while True:\n",
        "        result_response = requests.get(\n",
        "            f\"https://api.assemblyai.com/v2/transcript/{transcript_response['id']}\",\n",
        "            headers={'authorization': api_key}\n",
        "        ).json()\n",
        "        if result_response['status'] == 'completed':\n",
        "            return file_path, result_response['text']\n",
        "        elif result_response['status'] == 'error':\n",
        "            print(f\"Transcription failed for {file_path}: {result_response}\")\n",
        "            return file_path, None\n",
        "        time.sleep(10)\n",
        "\n",
        "# Directory with the audio files\n",
        "audio_dir = '/content/audio_files'\n",
        "\n",
        "# Directory to save the transcriptions\n",
        "transcript_dir = '/content/transcripts'\n",
        "os.makedirs(transcript_dir, exist_ok=True)\n",
        "\n",
        "# Create a list of all audio file paths\n",
        "audio_files = [os.path.join(audio_dir, filename) for filename in os.listdir(audio_dir) if filename.endswith('.wav')]\n",
        "\n",
        "# Create a ThreadPoolExecutor\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "    # Start transcription of all audio files and get an iterator of futures\n",
        "    futures = {executor.submit(transcribe_audio, file_path) for file_path in audio_files}\n",
        "\n",
        "    for future in concurrent.futures.as_completed(futures):\n",
        "        file_path, transcription = future.result()\n",
        "        print(f\"Transcription completed for {file_path}\")\n",
        "\n",
        "        # Only save the transcription if it was successful\n",
        "        if transcription is not None:\n",
        "            # Save the transcription to a file\n",
        "            transcript_file_path = os.path.join(transcript_dir, f\"{os.path.splitext(os.path.basename(file_path))[0]}.txt\")\n",
        "            with open(transcript_file_path, 'w') as f:\n",
        "                f.write(transcription)\n",
        "\n",
        "print(\"All transcriptions saved successfully!\")\n"
      ],
      "metadata": {
        "id": "tTN3zmnHD-Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data PreProcessing"
      ],
      "metadata": {
        "id": "8Mwp-D37_XEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Install Dependencies"
      ],
      "metadata": {
        "id": "isiPOqKFAJOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install dependencies\n",
        "%%bash\n",
        "pip install haystack-ai\n",
        "pip install \"sentence-transformers>=2.2.0\" \"huggingface_hub>=0.22.0\" transformers\n",
        "pip install markdown-it-py mdit_plain pypdf\n",
        "pip install gdown\n",
        "pip install --upgrade --quiet langchain-googledrive unstructured -q"
      ],
      "metadata": {
        "id": "GLP9z4ldAH7S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "001b4a9f-003a-4241-bea1-a75befec969e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting haystack-ai\n",
            "  Downloading haystack_ai-2.0.1-py3-none-any.whl (266 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 266.7/266.7 kB 4.3 MB/s eta 0:00:00\n",
            "Collecting boilerpy3 (from haystack-ai)\n",
            "  Downloading boilerpy3-1.0.7-py3-none-any.whl (22 kB)\n",
            "Collecting haystack-bm25 (from haystack-ai)\n",
            "  Downloading haystack_bm25-1.0.2-py2.py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (3.1.3)\n",
            "Collecting lazy-imports (from haystack-ai)\n",
            "  Downloading lazy_imports-0.3.1-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (10.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (1.25.2)\n",
            "Collecting openai>=1.1.0 (from haystack-ai)\n",
            "  Downloading openai-1.20.0-py3-none-any.whl (292 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 292.8/292.8 kB 8.8 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (2.0.3)\n",
            "Collecting posthog (from haystack-ai)\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.3/41.3 kB 2.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (2.8.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (2.31.0)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (8.2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from haystack-ai) (4.11.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->haystack-ai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai>=1.1.0->haystack-ai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.6/75.6 kB 7.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->haystack-ai) (2.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->haystack-ai) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog->haystack-ai)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog->haystack-ai)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->haystack-ai) (2024.2.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->haystack-ai) (1.2.0)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.9/77.9 kB 7.1 MB/s eta 0:00:00\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 6.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->haystack-ai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->haystack-ai) (2.16.3)\n",
            "Installing collected packages: monotonic, lazy-imports, haystack-bm25, h11, boilerpy3, backoff, posthog, httpcore, httpx, openai, haystack-ai\n",
            "Successfully installed backoff-2.2.1 boilerpy3-1.0.7 h11-0.14.0 haystack-ai-2.0.1 haystack-bm25-1.0.2 httpcore-1.0.5 httpx-0.27.0 lazy-imports-0.3.1 monotonic-1.6 openai-1.20.0 posthog-3.5.0\n",
            "Collecting sentence-transformers>=2.2.0\n",
            "  Downloading sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 163.3/163.3 kB 2.9 MB/s eta 0:00:00\n",
            "Collecting huggingface_hub>=0.22.0\n",
            "  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 388.9/388.9 kB 7.8 MB/s eta 0:00:00\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.0) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.0) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.0) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.0) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.0) (1.11.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.0) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.22.0) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.22.0) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.22.0) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.22.0) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.22.0) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.22.0) (4.11.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=2.2.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=2.2.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=2.2.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers>=2.2.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers>=2.2.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers>=2.2.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers>=2.2.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers>=2.2.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers>=2.2.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11.0->sentence-transformers>=2.2.0)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=2.2.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers>=2.2.0)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.22.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.22.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.22.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.22.0) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.2.0) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.2.0) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.2.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.2.0) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface_hub, nvidia-cusolver-cu12, sentence-transformers\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed huggingface_hub-0.22.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 sentence-transformers-2.6.1\n",
            "Requirement already satisfied: markdown-it-py in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
            "Collecting mdit_plain\n",
            "  Downloading mdit_plain-1.0.1-py3-none-any.whl (3.1 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 290.4/290.4 kB 4.8 MB/s eta 0:00:00\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py) (0.1.2)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.11.0)\n",
            "Installing collected packages: pypdf, mdit_plain\n",
            "Successfully installed mdit_plain-1.0.1 pypdf-4.2.0\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.4)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.9/1.9 MB 10.3 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.6/12.6 MB 38.2 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.9/1.9 MB 63.4 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 289.1/289.1 kB 33.7 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 433.8/433.8 kB 42.8 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 274.7/274.7 kB 33.9 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 981.5/981.5 kB 52.6 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 74.6 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 113.7/113.7 kB 15.5 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.4/49.4 kB 7.5 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 8.5 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141.1/141.1 kB 18.6 MB/s eta 0:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Data\n",
        "\n",
        "Here we download necessary files from a Google Drive folder but you can changeup the link to anything you like. The current folder contains the YT transcripts we transcribed earlier."
      ],
      "metadata": {
        "id": "Ijan7jdwANv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Download all files\n",
        "import gdown\n",
        "\n",
        "url = \"https://drive.google.com/drive/folders/1M8qvR0hTYH-qP_U43EbVhj5aFE_u8djW?usp=sharing\"\n",
        "output_dir = \"transcripts\"\n",
        "\n",
        "gdown.download_folder(url, quiet=True, output=output_dir, remaining_ok=True)"
      ],
      "metadata": {
        "id": "LgTYjLA0ATyR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8de7a28-5f08-4485-ae0f-1781b8104c37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['transcripts/1-hack-to-distribute-your-social-media-content-with-chris-josephs-autopilot.txt',\n",
              " 'transcripts/1-way-to-instantly-boost-productivity.txt',\n",
              " 'transcripts/3-non-obvious-networking-strategies-that-work.txt',\n",
              " 'transcripts/3-things-they-dont-tell-you-about-solopreneurship.txt',\n",
              " 'transcripts/4-steps-to-become-a-multipreneur.txt',\n",
              " 'transcripts/5-signs-multipreneurship-isnt-for-you.txt',\n",
              " 'transcripts/7-tools-i-used-to-build-a-million-dollar-business.txt',\n",
              " 'transcripts/30m-by-age-19-and-where-you-should-build-today.txt',\n",
              " 'transcripts/50-realizations-that-changed-my-life.txt',\n",
              " 'transcripts/ai-will-make-us-relearn-everything-we-do-explains-dave-rogenmoser-jasper-co-founder.txt',\n",
              " 'transcripts/alexis-ohanian-explains-the-importance-of-minimum-viable-community.txt',\n",
              " 'transcripts/alexis-ohanians-5-year-predictions-where-it-happens-podcast.txt',\n",
              " 'transcripts/all-teams-need-a-nerd-in-residence-explains-theo-tabah-late-checkout.txt',\n",
              " 'transcripts/amazons-unfair-advantage-with-ai-nik-sharma.txt',\n",
              " 'transcripts/andrew-wilkinson-explains-what-he-did-after-his-dopamine-detox.txt',\n",
              " 'transcripts/andrew-wilkinson-explains-why-he-doesnt-read-the-news-everyday.txt',\n",
              " 'transcripts/anything-is-possible-with-dave-friedberg-where-it-happens.txt',\n",
              " 'transcripts/are-we-alone-in-the-universe-with-tim-urban.txt',\n",
              " 'transcripts/are-you-draining-or-charging-your-batteries.txt',\n",
              " 'transcripts/audience-driven-content-how-your-community-shapes-your-podcasts-success-with-mad-realities.txt',\n",
              " 'transcripts/avoid-building-cookie-cutter-products-do-this-instead.txt',\n",
              " 'transcripts/avoid-this-huge-mistake-when-becoming-an-entrepreneur.txt',\n",
              " 'transcripts/be-unapologetic-about-what-you-want-with-anthony-pompliano.txt',\n",
              " 'transcripts/best-platform-to-build-an-audience.txt',\n",
              " 'transcripts/betting-big-on-web3-with-li-jin.txt',\n",
              " 'transcripts/beyond-fast-paced-editing-capturing-attention-in-a-content-driven-era.txt',\n",
              " 'transcripts/beyond-profit-how-to-prioritize-purpose-in-every-venture.txt',\n",
              " 'transcripts/billion-dollar-acquisitions-private-jets-and-the-art-science-of-persuasion.txt',\n",
              " 'transcripts/bootstrapping-an-eight-figure-business-michael-martocci-swagup.txt',\n",
              " 'transcripts/bootstrapping-through-a-bear-market-with-noah-kagan.txt',\n",
              " 'transcripts/brainstorming-1m-business-ideas-with-tiago-forte.txt',\n",
              " 'transcripts/brainstorming-the-most-profitable-ai-business-ideas.txt',\n",
              " 'transcripts/build-an-audience-using-memes-explains-chris-josephs-autopilot.txt',\n",
              " 'transcripts/build-culture-not-a-cult-explains-theo-tabah-late-checkout.txt',\n",
              " 'transcripts/build-once-sell-twice-with-visualize-values-jack-butcher.txt',\n",
              " 'transcripts/building-a-business-ready-to-sell-with-the-rideshare-guy.txt',\n",
              " 'transcripts/building-a-youtube-empire-unveiling-the-ultimate-growth-hacks.txt',\n",
              " 'transcripts/building-big-businesses-with-creators-with-emily-herrera.txt',\n",
              " 'transcripts/building-from-scratch-with-ryan-hoover.txt',\n",
              " 'transcripts/building-the-right-product-at-the-right-time-for-the-right-people-with-jack-butcher.txt',\n",
              " 'transcripts/building-trust-on-youtube-navigating-the-challenge-of-a-new-audience.txt',\n",
              " 'transcripts/bullish-or-bearish-on-meta-facebook-with-nikita-bier-where-it-happens-podcast.txt',\n",
              " 'transcripts/buying-happiness-or-selling-sadness-andrew-wilkinson.txt',\n",
              " 'transcripts/can-we-really-connect-with-ai-generated-art-with-dave-rogenmoser-jasper-co-founder.txt',\n",
              " 'transcripts/can-you-have-too-much-money.txt',\n",
              " 'transcripts/codie-sanchez-reveals-the-laziest-way-to-buy-a-business.txt',\n",
              " 'transcripts/coding-might-not-be-the-future-heres-why-with-makerpad-founder-ben-tossell.txt',\n",
              " 'transcripts/common-traits-of-legends-with-ndamukong-suh.txt',\n",
              " 'transcripts/content-creation-creators-vs-companies-with-jt-barnett-barnettx-creatorx.txt',\n",
              " 'transcripts/creating-content-is-more-profitable-than-software.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Index Documents\n",
        "\n",
        " Here we setup the pipeline for indexing documents (PDF, TXT, and MD files) and storing them in an in-memory document store."
      ],
      "metadata": {
        "id": "zyzRbsLXAYgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.components.writers import DocumentWriter\n",
        "from haystack.components.converters import MarkdownToDocument, PyPDFToDocument, TextFileToDocument\n",
        "from haystack.components.preprocessors import DocumentSplitter, DocumentCleaner\n",
        "from haystack.components.routers import FileTypeRouter\n",
        "from haystack.components.joiners import DocumentJoiner\n",
        "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
        "from haystack import Pipeline\n",
        "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
        "\n",
        "document_store = InMemoryDocumentStore()\n",
        "file_type_router = FileTypeRouter(mime_types=[\"text/plain\", \"application/pdf\", \"text/markdown\"])\n",
        "text_file_converter = TextFileToDocument()\n",
        "markdown_converter = MarkdownToDocument()\n",
        "pdf_converter = PyPDFToDocument()\n",
        "document_joiner = DocumentJoiner()\n",
        "\n",
        "#remove whitespace\n",
        "document_cleaner = DocumentCleaner()\n",
        "#breaks text into chunks with overlap to avoid missing context\n",
        "document_splitter = DocumentSplitter(split_by=\"word\", split_length=150, split_overlap=50)\n",
        "\n",
        "#Embeddings from documents\n",
        "document_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "document_writer = DocumentWriter(document_store)"
      ],
      "metadata": {
        "id": "vIe2FHsxAf7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Pipeline"
      ],
      "metadata": {
        "id": "F_bXJqvD_b3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Index Pipeline\n",
        "Here we index the pipeline by connecting the components and running the pipeline."
      ],
      "metadata": {
        "id": "Sow2nNYJAys4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#indexing pipeline\n",
        "preprocessing_pipeline = Pipeline()\n",
        "preprocessing_pipeline.add_component(instance=file_type_router, name=\"file_type_router\")\n",
        "preprocessing_pipeline.add_component(instance=text_file_converter, name=\"text_file_converter\")\n",
        "preprocessing_pipeline.add_component(instance=markdown_converter, name=\"markdown_converter\")\n",
        "preprocessing_pipeline.add_component(instance=pdf_converter, name=\"pypdf_converter\")\n",
        "preprocessing_pipeline.add_component(instance=document_joiner, name=\"document_joiner\")\n",
        "preprocessing_pipeline.add_component(instance=document_cleaner, name=\"document_cleaner\")\n",
        "preprocessing_pipeline.add_component(instance=document_splitter, name=\"document_splitter\")\n",
        "preprocessing_pipeline.add_component(instance=document_embedder, name=\"document_embedder\")\n",
        "preprocessing_pipeline.add_component(instance=document_writer, name=\"document_writer\")\n",
        "\n",
        "#connecting the pipeline\n",
        "preprocessing_pipeline.connect(\"file_type_router.text/plain\", \"text_file_converter.sources\")\n",
        "preprocessing_pipeline.connect(\"file_type_router.application/pdf\", \"pypdf_converter.sources\")\n",
        "preprocessing_pipeline.connect(\"file_type_router.text/markdown\", \"markdown_converter.sources\")\n",
        "preprocessing_pipeline.connect(\"text_file_converter\", \"document_joiner\")\n",
        "preprocessing_pipeline.connect(\"pypdf_converter\", \"document_joiner\")\n",
        "preprocessing_pipeline.connect(\"markdown_converter\", \"document_joiner\")\n",
        "preprocessing_pipeline.connect(\"document_joiner\", \"document_cleaner\")\n",
        "preprocessing_pipeline.connect(\"document_cleaner\", \"document_splitter\")\n",
        "preprocessing_pipeline.connect(\"document_splitter\", \"document_embedder\")\n",
        "preprocessing_pipeline.connect(\"document_embedder\", \"document_writer\")\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "preprocessing_pipeline.run({\"file_type_router\": {\"sources\": list(Path(output_dir).glob(\"**/*\"))}})"
      ],
      "metadata": {
        "id": "7lUQn-jgA53W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question Answering Pipeline\n",
        "\n",
        "Here we use the Mistral-7B-Instruct-v0.1 model but you can replace it anything you like."
      ],
      "metadata": {
        "id": "TFMgGDm6BGUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "if \"HF_API_TOKEN\" not in os.environ:\n",
        "    os.environ[\"HF_API_TOKEN\"] = getpass(\"Enter Hugging Face token:\")\n",
        "\n",
        "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
        "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
        "from haystack.components.builders import PromptBuilder\n",
        "from haystack.components.generators import HuggingFaceTGIGenerator\n",
        "\n",
        "template = \"\"\"\n",
        "Answer the questions based on the given context.\n",
        "\n",
        "Context:\n",
        "{% for document in documents %}\n",
        "    {{ document.content }}\n",
        "{% endfor %}\n",
        "\n",
        "Question: {{ question }}\n",
        "Answer:\n",
        "\"\"\"\n",
        "pipe = Pipeline()\n",
        "pipe.add_component(\"embedder\", SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\"))\n",
        "pipe.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store=document_store))\n",
        "pipe.add_component(\"prompt_builder\", PromptBuilder(template=template))\n",
        "pipe.add_component(\"llm\", HuggingFaceTGIGenerator(\"mistralai/Mistral-7B-Instruct-v0.1\"))\n",
        "\n",
        "pipe.connect(\"embedder.embedding\", \"retriever.query_embedding\")\n",
        "pipe.connect(\"retriever\", \"prompt_builder.documents\")\n",
        "pipe.connect(\"prompt_builder\", \"llm\")"
      ],
      "metadata": {
        "id": "pTqwHsUlBVFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat with the Data"
      ],
      "metadata": {
        "id": "2aFUj8V5BWJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = (\n",
        "    \"Give me 10 startup ideas\"\n",
        ")\n",
        "\n",
        "pipe.run(\n",
        "    {\n",
        "        \"embedder\": {\"text\": question},\n",
        "        \"prompt_builder\": {\"question\": question},\n",
        "        \"llm\": {\"generation_kwargs\": {\"max_new_tokens\": 350}},\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "3eJhaVFpBcMV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}