{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install necessary Packages**"
      ],
      "metadata": {
        "id": "sNNZG8jO5F_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf==3.14.0 --quiet\n",
        "!pip install tiktoken==0.4.0 --quiet\n",
        "!pip install langchain==0.0.353 --quiet\n",
        "!pip install openai==0.27.8 --quiet\n",
        "!pip install gdown==4.7.3 --quiet\n",
        "!pip install langchain-google-genai --quiet\n",
        "!pip install requests --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPan--YD5Jm0",
        "outputId": "3f331a3a-c1f4-41bd-f77a-7f3d68a81734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.8/269.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.1/803.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.6/252.6 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.4/252.4 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.8/250.8 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.4/246.4 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.1/242.1 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.3/241.3 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import required libraries**"
      ],
      "metadata": {
        "id": "Fnt1L46x5UMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import nltk\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.corpus import stopwords\n",
        "from pypdf import PdfReader\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "import requests"
      ],
      "metadata": {
        "id": "h3DA_iUU5XIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define processing function**"
      ],
      "metadata": {
        "id": "iP66syLL5Y1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text, stop_words):\n",
        "    \"\"\"\n",
        "    Tokenizes and preprocesses the input text, removing stopwords and short\n",
        "    tokens.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The input text to preprocess.\n",
        "        stop_words (set): A set of stopwords to be removed from the text.\n",
        "    Returns:\n",
        "        list: A list of preprocessed tokens.\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    for token in simple_preprocess(text, deacc=True):\n",
        "        if token not in stop_words and len(token) > 3:\n",
        "            result.append(token)\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "Hrd4tVBM5bJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define function to extract topics from PDF using LDA**"
      ],
      "metadata": {
        "id": "OUrle54J5coU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_lists_from_pdf(file, num_topics, words_per_topic):\n",
        "    \"\"\"\n",
        "    Extracts topics and their associated words from a PDF document using the\n",
        "    Latent Dirichlet Allocation (LDA) algorithm.\n",
        "\n",
        "    Parameters:\n",
        "        file (str): The path to the PDF file for topic extraction.\n",
        "        num_topics (int): The number of topics to discover.\n",
        "        words_per_topic (int): The number of words to include per topic.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of num_topics sublists, each containing relevant words\n",
        "        for a topic.\n",
        "    \"\"\"\n",
        "    # Load the pdf file\n",
        "    loader = PdfReader(file)\n",
        "\n",
        "    # Extract the text from each page into a list. Each page is considered a document\n",
        "    documents= []\n",
        "    for page in loader.pages:\n",
        "        documents.append(page.extract_text())\n",
        "\n",
        "    # Preprocess the documents\n",
        "    nltk.download('stopwords')\n",
        "    stop_words = set(stopwords.words(['english','spanish']))\n",
        "    processed_documents = [preprocess(doc, stop_words) for doc in documents]\n",
        "\n",
        "    # Create a dictionary and a corpus\n",
        "    dictionary = corpora.Dictionary(processed_documents)\n",
        "    corpus = [dictionary.doc2bow(doc) for doc in processed_documents]\n",
        "\n",
        "    # Build the LDA model\n",
        "    lda_model = LdaModel(\n",
        "        corpus,\n",
        "        num_topics=num_topics,\n",
        "        id2word=dictionary,\n",
        "        passes=15\n",
        "        )\n",
        "\n",
        "    # Retrieve the topics and their corresponding words\n",
        "    topics = lda_model.print_topics(num_words=words_per_topic)\n",
        "\n",
        "    # Store each list of words from each topic into a list\n",
        "    topics_ls = []\n",
        "    for topic in topics:\n",
        "        words = topic[1].split(\"+\")\n",
        "        topic_words = [word.split(\"*\")[1].replace('\"', '').strip() for word in words]\n",
        "        topics_ls.append(topic_words)\n",
        "\n",
        "    return topics_ls"
      ],
      "metadata": {
        "id": "x7lU29Po5e3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define function to generate prompts for LLM based on extracted topics**"
      ],
      "metadata": {
        "id": "uDqhO74_5gjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def topics_from_pdf(llm, file, num_topics, words_per_topic):\n",
        "    \"\"\"\n",
        "    Generates descriptive prompts for LLM based on topic words extracted from a\n",
        "    PDF document.\n",
        "\n",
        "    This function takes the output of `get_topic_lists_from_pdf` function,\n",
        "    which consists of a list of topic-related words for each topic, and\n",
        "    generates an output string in bulleted nested list format.\n",
        "\n",
        "    Parameters:\n",
        "        llm (LLM): An instance of the Large Language Model (LLM) for generating\n",
        "        responses.\n",
        "        file (str): The path to the PDF file for extracting topic-related words.\n",
        "        num_topics (int): The number of topics to consider.\n",
        "        words_per_topic (int): The number of words per topic to include.\n",
        "\n",
        "    Returns:\n",
        "        str: A response generated by the language model based on the provided\n",
        "        topic words.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract topics and convert them to string\n",
        "    list_of_topicwords = get_topic_lists_from_pdf(file, num_topics,\n",
        "                                                  words_per_topic)\n",
        "    string_lda = \"\"\n",
        "    for lst in list_of_topicwords:\n",
        "        string_lda += str(lst) + \"\\n\"\n",
        "\n",
        "    # Create the template\n",
        "    template_string = '''Describe the topic of each of the {num_topics}\n",
        "        double-quote delimited lists in a simple sentence and also write down\n",
        "        three possible different subthemes. The lists are the result of an\n",
        "        algorithm for topic discovery.\n",
        "        Do not provide an introduction or a conclusion, only describe the\n",
        "        topics. Do not mention the word \"topic\" when describing the topics.\n",
        "        Use the following template for the response.\n",
        "\n",
        "        1: <<<(sentence describing the topic)>>>\n",
        "        - <<<(Phrase describing the first subtheme)>>>\n",
        "        - <<<(Phrase describing the second subtheme)>>>\n",
        "        - <<<(Phrase describing the third subtheme)>>>\n",
        "\n",
        "        2: <<<(sentence describing the topic)>>>\n",
        "        - <<<(Phrase describing the first subtheme)>>>\n",
        "        - <<<(Phrase describing the second subtheme)>>>\n",
        "        - <<<(Phrase describing the third subtheme)>>>\n",
        "\n",
        "        ...\n",
        "\n",
        "        n: <<<(sentence describing the topic)>>>\n",
        "        - <<<(Phrase describing the first subtheme)>>>\n",
        "        - <<<(Phrase describing the second subtheme)>>>\n",
        "        - <<<(Phrase describing the third subtheme)>>>\n",
        "\n",
        "        Lists: \"\"\"{string_lda}\"\"\" '''\n",
        "\n",
        "    # LLM call\n",
        "    prompt_template = ChatPromptTemplate.from_template(template_string)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "    response = chain.run({\n",
        "        \"string_lda\" : string_lda,\n",
        "        \"num_topics\" : num_topics\n",
        "        })\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "BomYUpE35kF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configure and use Google Generative AI**"
      ],
      "metadata": {
        "id": "9Fy9XDhA5ll6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get(\"google_api_key\")\n",
        "\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "wl8i3kdi51qH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**List available models and instantiate Google Generative AI model**"
      ],
      "metadata": {
        "id": "C64TCC8052zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List available models\n",
        "for m in genai.list_models():\n",
        "    if 'generateContent' in m.supported_generation_methods:\n",
        "        print(m.name)\n",
        "\n",
        "# Instantiate Google Generative AI model\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=api_key)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "izIBmhBa54yV",
        "outputId": "8bcbc290-28ce-463d-a213-1dde2f8b053a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.0-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro\n",
            "models/gemini-pro-vision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download PDF file**"
      ],
      "metadata": {
        "id": "ecp8CD2F56bS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://ia800506.us.archive.org/15/items/BOUNDARIESTheBook/BOUNDARIES%20The%20Book.pdf\"\n",
        "\n",
        "# Define the output file name and path\n",
        "file = \"./boundaries.pdf\"\n",
        "\n",
        "# Send a GET request to the URL and save the response content as a binary file\n",
        "response = requests.get(url)\n",
        "with open(file, \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "num_topics = 6\n",
        "words_per_topic = 30"
      ],
      "metadata": {
        "id": "awT38aw_5-dW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extract topics and generate summary**"
      ],
      "metadata": {
        "id": "TRMxBpal6AA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract topics and generate summary\n",
        "summary = topics_from_pdf(llm, file, num_topics, words_per_topic)\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAZ28TeB6COk",
        "outputId": "4f59e517-142b-42dd-edcb-aecca118df57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: Ways to establish boundaries in different aspects of life.\n",
            "- Establishing boundaries in personal relationships.\n",
            "- Establishing boundaries at work.\n",
            "- Establishing boundaries with family members.\n",
            "\n",
            "2: The importance of setting boundaries in various relationships.\n",
            "- The benefits of setting boundaries with children.\n",
            "- The challenges of setting boundaries with parents.\n",
            "- The role of boundaries in healthy romantic relationships.\n",
            "\n",
            "3: The impact of boundaries on personal well-being.\n",
            "- The importance of setting boundaries to protect one's emotional health.\n",
            "- The challenges of setting boundaries with people who are close to you.\n",
            "- The benefits of setting boundaries for personal growth.\n",
            "\n",
            "4: The challenges of setting and maintaining boundaries.\n",
            "- The fear of hurting others' feelings.\n",
            "- The guilt associated with saying no.\n",
            "- The pressure to conform to societal expectations.\n",
            "\n",
            "5: The benefits of setting and maintaining boundaries.\n",
            "- Improved self-esteem.\n",
            "- Increased self-awareness.\n",
            "- Stronger relationships.\n",
            "\n",
            "6: The different types of boundaries.\n",
            "- Physical boundaries.\n",
            "- Emotional boundaries.\n",
            "- Mental boundaries.\n"
          ]
        }
      ]
    }
  ]
}